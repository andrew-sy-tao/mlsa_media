{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports all needed libraries\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import gzip\n",
    "\n",
    "import re\n",
    "import texthero as hero\n",
    "from texthero import preprocessing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Dropout, PReLU, GRU, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation and Training/Testing of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports local training/testing dataset\n",
    "# Clears unwanted columns\n",
    "\n",
    "df = pd.read_csv(input('input training/testing data'), encoding = \"UTF-8\")\n",
    "\n",
    "df = df.drop(['Num', 'Date', 'Query', 'User_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows graph of data divided between positive and negatives sentiments\n",
    "\n",
    "val_count = df.Sentiment.value_counts()\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(val_count.index, val_count.values)\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.axis(['Negative', 'Positive', 0, 200000])\n",
    "plt.title(\"Sentiment Data Distribution\")\n",
    "\n",
    "plt.savefig('training_data_chart.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace username with USERNAME\n",
    "\n",
    "def pre_cleaning(text):\n",
    "    line=re.sub(r\"\\B@\\w+\", \"\", text) \n",
    "    return line\n",
    "\n",
    "# Uses TextHero package to:\n",
    "# 1) Remove non-assigned values\n",
    "# 2) Lowercase all text\n",
    "# 3) Remove digits\n",
    "# 4) Remove punctuation\n",
    "# 5) Remove stopwords\n",
    "# 6) Remove whitespace\n",
    "# 7) Remove urls\n",
    "\n",
    "custom_pipeline = [preprocessing.fillna,\n",
    "                   preprocessing.lowercase,\n",
    "                   preprocessing.remove_digits,\n",
    "                   preprocessing.remove_punctuation,\n",
    "                   preprocessing.remove_stopwords,\n",
    "                   preprocessing.remove_whitespace,\n",
    "                   preprocessing.remove_urls,\n",
    "                  ]\n",
    "\n",
    "df.Text = df.Text.apply(lambda x: pre_cleaning(x))\n",
    "df['Text'] = hero.clean(df['Text'], custom_pipeline)\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uses natural language toolkit lemmatization library to lemmatize data\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Gets part of speech\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Lemmatizes\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text.split()]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['Text'] = df.Text.apply(lemmatize_text)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits dataset into 80% training data, and 20% testing data\n",
    "\n",
    "test_size = 0.2\n",
    "train_data, test_data = train_test_split(df, test_size=test_size, random_state=42, shuffle=True)\n",
    "print(\"Test Data size\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms data into vectors (Tfidf preserves and takes into account frequency of features)\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=7500, min_df=20, max_df=0.3, ngram_range=(1,2))\n",
    "vectorizer.fit(train_data.Text.to_list())\n",
    "x_train = vectorizer.transform(train_data.Text.to_list())\n",
    "x_test = vectorizer.transform(test_data.Text.to_list())\n",
    "\n",
    "print(\"Training X Shape:\", x_train.shape)\n",
    "print(\"Testing X Shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves vectorizer\n",
    "\n",
    "pickle.dump(vectorizer, open(\"vectorizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test labels\n",
    "\n",
    "Y_train = train_data['Sentiment']\n",
    "Y_test = test_data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms the data type to array form and reshapes it into a tensor array\n",
    "\n",
    "X_train = scipy.sparse.csr_matrix.toarray(x_train)\n",
    "X_test = scipy.sparse.csr_matrix.toarray(x_test)\n",
    "\n",
    "X_train = X_train.reshape(x_train.shape[0], 1, x_train.shape[1])\n",
    "X_test = X_test.reshape(x_test.shape[0], 1, x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creates the actual Model using Keras\n",
    "\n",
    "Model_Final = Sequential()\n",
    "\n",
    "Model_Final.add(LSTM(32, input_shape=(1, X_train.shape[2]), return_sequences=True))\n",
    "Model_Final.add(PReLU())\n",
    "Model_Final.add(Dropout(0.5))\n",
    "\n",
    "Model_Final.add(LSTM(32))\n",
    "Model_Final.add(PReLU())\n",
    "Model_Final.add(Dropout(0.5))\n",
    "\n",
    "Model_Final.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt=keras.optimizers.Adam(0.025)\n",
    "\n",
    "Model_Final.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "Model_Final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "history = Model_Final.fit(X_train, Y_train, epochs=10, batch_size=256, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes model history to csv\n",
    "\n",
    "Model_Hist = 'Model_Hist.csv'\n",
    "            \n",
    "with open(Model_Hist, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in history.history:\n",
    "        writer.writerow(history.history[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs model history using matplotlib\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves model/parameters to file\n",
    "\n",
    "Model_Final.save(\"Full_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "Model_Final.evaluate(X_test, Y_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Model with Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a function to load the Target Tweets (json) file\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    data = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip('\\n|\\r')))\n",
    "    print('Loaded {} records from {}'.format(len(data), input_path))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used an independent twitter hydrator app before this to get fully hydrated tweets from dataset\n",
    "# This code retrieves the fully hydrated tweets from a local file\n",
    "\n",
    "file = load_jsonl(input('input target dataset'))\n",
    "\n",
    "# Creates dataframe from date and text values for each tweet\n",
    "db_data = []\n",
    "db_cols = ['Dates', 'Uncleaned_Text']\n",
    "for f in file:\n",
    "    Date = f.get('created_at')\n",
    "    Text = f.get(\"full_text\")\n",
    "    db_data.append([Date, Text])\n",
    "df = pd.DataFrame(db_data, columns=db_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloads vectorizer and model\n",
    "\n",
    "vectorizer = pickle.load(open(input('input vectorizer file path'), 'rb'))\n",
    "model = keras.models.load_model(input('input model file path'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return correct format of dates from csv dates column\n",
    "\n",
    "def date_parser(text):\n",
    "    p = re.compile('^[A-Za-z]+\\s[A-Za-z]+\\s([0-9]+)')\n",
    "    return str(p.findall(text)[0]) + '/2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces dates column values with correct format\n",
    "\n",
    "Twitter_Target_df.Dates = Twitter_Target_df.Dates.apply(lambda x: date_parser(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesses target dataset text in the same way as used before\n",
    "\n",
    "Twitter_Target_df['Cleaned_Text'] = Twitter_Target_df['Uncleaned_Text']\n",
    "Twitter_Target_df.Cleaned_Text = Twitter_Target_df.Cleaned_Text.apply(lambda x: pre_cleaning(x))\n",
    "Twitter_Target_df['Cleaned_Text'] = hero.clean(Twitter_Target_df['Cleaned_Text'], custom_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizes the target dataset text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "Twitter_Target_df['Cleaned_Text'] = Twitter_Target_df.Cleaned_Text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizes the text\n",
    "\n",
    "Tweets = vectorizer.transform(Twitter_Target_df.Cleaned_Text.to_list())\n",
    "\n",
    "Tweets = scipy.sparse.csr_matrix.toarray(Tweets)\n",
    "\n",
    "Tweets = Tweets.reshape(Tweets.shape[0], 1, Tweets.shape[1])\n",
    "\n",
    "print(\"Tweets Shape:\", Tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes the predictions!\n",
    "\n",
    "Predictions = Model_Final.predict(Tweets)\n",
    "\n",
    "Twitter_Target_df['Sentiment_Confidence'] = Predictions\n",
    "\n",
    "Twitter_Target_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts the predicted sentiment labels into positive or negative based on sentiment scores. Who even needs list comps :P\n",
    "\n",
    "Sents = []\n",
    "for value in Twitter_Target_df['Sentiment_Confidence']:\n",
    "    if value > 0.5:\n",
    "        Sents.append('Positive')\n",
    "    elif value <= 0.5:\n",
    "        Sents.append('Negative')\n",
    "        \n",
    "Twitter_Target_df['Sentiment'] = Sents\n",
    "Twitter_Target_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Twitter_Target_df.to_csv('input output file name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a dictionary version of the pandas dataframe\n",
    "\n",
    "tdict = {}\n",
    "for i, j in Twitter_Target_df.iterrows(): \n",
    "    date = j['Dates']\n",
    "    sentiment = j['Sentiment']\n",
    "    if date not in tdict:\n",
    "        tdict[date] = [sentiment]\n",
    "    tdict[date].append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds number of tweets per day and displays\n",
    "\n",
    "def Num_Of_Tweets_Per_Day(Dataset):\n",
    "    odict = {}\n",
    "    for k, v in Dataset.items():\n",
    "        odict[k] = len(v)\n",
    "    return odict\n",
    "\n",
    "Nummin = Num_Of_Tweets_Per_Day(tdict)\n",
    "\n",
    "for date, num in Nummin.items():\n",
    "    print(str(num) + ' tweets on ' + str(date))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts tweets into being either positive or negative sentiment\n",
    "\n",
    "def Sorter(Dataset):\n",
    "    odict = {}\n",
    "    for k, v in Dataset.items():\n",
    "        Sents = {'pos': 0, 'neg': 0}\n",
    "        for sent in v:\n",
    "            if sent == 'Positive':\n",
    "                Sents['pos'] += 1\n",
    "            else:\n",
    "                Sents['neg'] += 1\n",
    "        percentage = Sents['pos']/(Sents['pos'] + Sents['neg'])\n",
    "        odict[k] = percentage\n",
    "    return odict\n",
    "        \n",
    "FINAL = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fdict = Sorter(tdict)\n",
    "FINAL.update(Fdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(FINAL.keys(), FINAL.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes daily sentiment percentage scores out to csv\n",
    "\n",
    "w = csv.writer(open(\"FINAL.csv\", \"w\"))\n",
    "for key, val in FINAL.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rereads previously written dataset\n",
    "\n",
    "Wanted = {}\n",
    "\n",
    "with open(input('input file path of wanted csv', 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for k, v in reader:\n",
    "        Wanted[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorts into data by month (for better visualization)\n",
    "\n",
    "def Month_Sorter(Dataset, Month):\n",
    "    monthly_dict = {}\n",
    "    for k, v in Dataset.items():\n",
    "        if Month in k:\n",
    "            monthly_dict[k] = v\n",
    "    return monthly_dict\n",
    "\n",
    "March_Data = Month_Sorter(FINAL, 'Mar')\n",
    "April_Data = Month_Sorter(FINAL, 'Apr')\n",
    "May_Data = Month_Sorter(FINAL, 'May')\n",
    "June_Data = Month_Sorter(FINAL, 'Jun')\n",
    "July_Data = Month_Sorter(FINAL, 'Jul')\n",
    "August_Data = Month_Sorter(FINAL, 'Aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Graphs barchart of final daily sentiment percentage scores\n",
    "\n",
    "plt.bar(FINAL.keys(), FINAL.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
